{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tensor Tutorial\n",
    "\n",
    "PyTorch is a popular deep learning library that provides tensor computation with GPU acceleration and automatic differentiation capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operations\n",
    "\n",
    "Let's start by exploring the fundamental operations with PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Creating a basic PyTorch tensor\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(\"Basic tensor:\", tensor)\n",
    "\n",
    "# Tensor Operations\n",
    "squared_tensor = tensor ** 2     # Element-wise squaring\n",
    "mean_value = tensor.mean()       # Calculate mean\n",
    "sum_value = tensor.sum()         # Calculate sum\n",
    "\n",
    "print(\"\\nSquared tensor:\", squared_tensor)\n",
    "print(\"Mean value:\", mean_value.item())\n",
    "print(\"Sum value:\", sum_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors with specific properties\n",
    "zeros = torch.zeros(3, 4)                 # Tensor of zeros\n",
    "ones = torch.ones(2, 3)                   # Tensor of ones\n",
    "random_tensor = torch.rand(2, 3)          # Random values between 0 and 1\n",
    "range_tensor = torch.arange(0, 10, 2)     # Range with step size\n",
    "\n",
    "print(\"Zeros tensor:\\n\", zeros)\n",
    "print(\"\\nOnes tensor:\\n\", ones)\n",
    "print(\"\\nRandom tensor:\\n\", random_tensor)\n",
    "print(\"\\nRange tensor:\", range_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping and Manipulating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "original = torch.arange(6)\n",
    "print(\"Original tensor:\", original)\n",
    "\n",
    "reshaped = original.reshape(2, 3)\n",
    "print(\"\\nReshaped to 2x3:\\n\", reshaped)\n",
    "\n",
    "flattened = reshaped.flatten()\n",
    "print(\"\\nFlattened back:\", flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation\n",
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([4, 5, 6])\n",
    "\n",
    "concat_result = torch.cat([tensor1, tensor2])\n",
    "print(\"Concatenated tensors:\", concat_result)\n",
    "\n",
    "stacked_result = torch.stack([tensor1, tensor2])  # New dimension\n",
    "print(\"\\nStacked tensors:\\n\", stacked_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing\n",
    "values = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "print(\"Original tensor:\", values)\n",
    "\n",
    "subset = values[2:5]\n",
    "print(\"Subset (elements 2-4):\", subset)\n",
    "\n",
    "# Boolean masking\n",
    "mask = values > 3\n",
    "print(\"\\nMask (values > 3):\", mask)\n",
    "filtered = values[mask]  # Returns tensor([4, 5, 6])\n",
    "print(\"Filtered values:\", filtered)\n",
    "\n",
    "# Finding indices matching condition\n",
    "indices = torch.where(values % 2 == 0)\n",
    "print(\"\\nIndices of even numbers:\", indices)\n",
    "print(\"Even values:\", values[indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Between NumPy, Pandas, and PyTorch\n",
    "\n",
    "PyTorch provides seamless integration with NumPy arrays and Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NumPy array to PyTorch tensor\n",
    "np_array = np.array([1, 2, 3])\n",
    "tensor_from_np = torch.from_numpy(np_array)\n",
    "print(\"NumPy array:\", np_array)\n",
    "print(\"Tensor from NumPy:\", tensor_from_np)\n",
    "print(f\"Type: {type(tensor_from_np)}\")\n",
    "\n",
    "# PyTorch tensor to NumPy array\n",
    "tensor = torch.tensor([4, 5, 6])\n",
    "np_from_tensor = tensor.numpy()\n",
    "print(\"\\nPyTorch tensor:\", tensor)\n",
    "print(\"NumPy from tensor:\", np_from_tensor)\n",
    "print(f\"Type: {type(np_from_tensor)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas DataFrame to PyTorch tensor\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "tensor_from_df = torch.tensor(df.values)\n",
    "print(\"Pandas DataFrame:\\n\", df)\n",
    "print(\"\\nTensor from DataFrame:\\n\", tensor_from_df)\n",
    "\n",
    "# PyTorch tensor to Pandas DataFrame\n",
    "tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "df_from_tensor = pd.DataFrame(tensor_2d.numpy())\n",
    "print(\"\\nPyTorch 2D tensor:\\n\", tensor_2d)\n",
    "print(\"\\nDataFrame from tensor:\\n\", df_from_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "One of PyTorch's most powerful features is its automatic differentiation system (autograd), which enables gradient-based optimization for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic autograd example\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)  # Enable gradient tracking\n",
    "print(\"x:\", x)\n",
    "\n",
    "y = x * x  # y = x^2\n",
    "print(\"y = x^2:\", y)\n",
    "\n",
    "z = y.sum()  # z = sum(y)\n",
    "print(\"z = sum(y):\", z)\n",
    "\n",
    "# Compute gradient of z with respect to x\n",
    "z.backward()\n",
    "\n",
    "# Access gradients\n",
    "print(\"\\nGradient of z with respect to x (should be 2*x):\\n\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient for Non-Scalar Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset gradients\n",
    "x = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "print(\"x:\\n\", x)\n",
    "\n",
    "y = x * x\n",
    "print(\"\\ny = x^2:\\n\", y)\n",
    "\n",
    "# For non-scalar outputs, specify gradient argument\n",
    "y.backward(torch.ones_like(y))  # Equivalent to sum() then backward()\n",
    "print(\"\\nGradient of y with respect to x (should be 2*x):\\n\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detaching Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset gradients\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "print(\"x:\", x)\n",
    "x.grad = None  # Clear gradients\n",
    "\n",
    "y = x * x\n",
    "print(\"y = x^2:\", y)\n",
    "\n",
    "# Detach from computation graph\n",
    "z = y.detach()  \n",
    "print(\"z = y.detach():\", z)\n",
    "\n",
    "# This has no effect on x.grad since gradient flow is stopped\n",
    "z.backward(torch.ones_like(z))\n",
    "print(\"\\nx.grad after z.backward():\", x.grad)  # Should be None\n",
    "\n",
    "# But this will affect x.grad\n",
    "y.sum().backward()\n",
    "print(\"x.grad after y.sum().backward():\", x.grad)  # Should be 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients with Control Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    y = x * 2\n",
    "    while y.norm() < 1000:\n",
    "        y = y * 2\n",
    "    return y\n",
    "\n",
    "x = torch.tensor([0.5], requires_grad=True)\n",
    "print(\"Initial x:\", x)\n",
    "\n",
    "y = f(x)\n",
    "print(\"\\nResult after function f(x):\", y)\n",
    "\n",
    "y.backward()\n",
    "print(\"\\nGradient of y with respect to x:\", x.grad)\n",
    "print(\"\\nRatio of y/x:\", y.item() / x.item())\n",
    "print(\"These values match, validating that the gradient is correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Performance Insights\n",
    "\n",
    "**Use NumPy for**:\n",
    "  - CPU-only computations\n",
    "  - Integration with scientific Python ecosystem\n",
    "  - Applications not requiring gradients\n",
    "\n",
    "**Use PyTorch for**:\n",
    "  - Deep learning models\n",
    "  - GPU acceleration\n",
    "  - Automatic differentiation\n",
    "  - Dynamic computational graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Check if GPU is available\n",
    "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    # Create a tensor on GPU\n",
    "    cuda_tensor = torch.tensor([1, 2, 3], device='cuda')\n",
    "    print(\"CUDA Tensor:\", cuda_tensor)\n",
    "    \n",
    "    # Move tensor to GPU\n",
    "    cpu_tensor = torch.tensor([4, 5, 6])\n",
    "    gpu_tensor = cpu_tensor.to('cuda')\n",
    "    print(\"Moved to GPU:\", gpu_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}